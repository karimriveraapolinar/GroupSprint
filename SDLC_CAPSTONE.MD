# SDLC Capstone — Sprint 1 Process Analysis  
**Course:** Software Quality Assurance  

**Sprint:** Sprint 1  

**Model Selected:** V-Model

**Submission Type:** Solo  

**Due:** End of class Thursday  

---

## Section 1 — Model Selection and Rationale

**Selected Model:** V-Model

**Rationale:** Even though we (my team) never explicitly said that we would work this way it is how it turned out, we would code something and then test that specific thing, but not test everything else as an Agile model would, we essentially took it stage by stage. We also pretty much stuck to our requirements we had agreed upon and the games skeleton there wasnt much or any scope creep at all. Our deadline was not pushed back or moved back, we had a strict deadline which is something not always true for agile modeled sprints.

---

## Section 2 — Phase Analysis

<!-- 
Walk through every phase of your chosen model. For each phase, address two things:  
1. What element of Sprint 1 corresponds to this phase, even loosely?  
2. What would this phase have looked like under strict model adherence?  

Every phase must be addressed. Phases that don't map cleanly are not a reason to skip — they are the most valuable part of the analysis. 
-->

### Phase: Sprint 1

**What Sprint 1 produced:**  We created a planning/requirement file, which included the basics on what we needed to have and include. There was a bit that was needed during the project, which wasn't outlined in this file.

**What strict adherence would have looked like:** We would have included requirements/test cases to help more statically define what the project needed to include 

---

### Phase: Sprint 1

**What Sprint 1 produced:**  The project was programmed in python and the game could be played in the terminal. This was confirmed by the programmer and was confirmed immediately after it was programmed. This meant there was no specific time to test or run QA. There were a lot of bugs found by the other group; they found the main 3, but then there were also a lot of others that we didn't plan to have that they found. This can be best explained by our lack of test cases and not being thorough with our own tests.

**What strict adherence would have looked like:**  We would have had someone else test the project and, most importantly, allocate a specific day and or time to strictly do QA only.

---

### Phase: Sprint 1

**What Sprint 1 produced:** We moved all around stage-wise, we moved up and down the stages in the sense that we were trying to improve things in most places. For example, we needed to add HTML to the project, which wasn't outlined in the requirements before, and now had to be added.

**What strict adherence would have looked like:**  

---

<!-- Add additional phase sections as needed for your chosen model -->

---

## Section 3 — Defect Case Studies

<!-- 
Select two bugs from your GitHub Issues filed during the Week 6 swap. For each bug provide:  
1. A description of the bug and what it caused  
2. The phase that introduced the defect (requirements / design / implementation)  
3. The phase that caught it — and whether your model would have caught it earlier  
4. The cost of catching it late  

Link directly to each GitHub Issue. 
-->

### Bug 1

**GitHub Issue:** [Issue 1](https://github.com/Gcc07/GroupSprint-QA/issues/1)  

**Description:**  

**Phase that introduced the defect:**  When we were programming, and also our own QA testing, since we didn't think of doing something as absurd as having a very long name, which is why the best QA comes from when you release it to the public, since you never really know what somebody can do and will do.

**Phase that caught it:**  The QA from another group, since we had to QA test another group, they had to QA test ours too.

**Would your chosen model have caught it earlier?** If it were used/implemented correctly, it would have been; unfortunately, we didn't implement it correctly.

**Cost of catching it late:**  Time constraints/having to fix these bugs before the sprint was over/due, so it added a very minuscule amount of stress. Other than that, luckily, since there wasn't money invested, the cost of catching it was only time and stress.

---

### Bug 2

**GitHub Issue:** [Issue 2](https://github.com/Gcc07/GroupSprint-QA/issues/6)

**Description:**  Very long names for pet names would result in the game becoming extremely laggy and even slowing down input.

**Phase that introduced the defect:** When we were programming, and also our own QA testing, since we didn't think of doing something as absurd as having a very long name, which is why the best QA comes from when you release it to the public, since you never really know what somebody can do and will do.

**Phase that caught it:**  The QA from another group, since we had to QA test another group, they had to QA test ours too.

**Would your chosen model have caught it earlier?** If it were used/implemented correctly, it would have been; unfortunately, we didn't implement it correctly.

**Cost of catching it late:**  Time constraints/having to fix these bugs before the sprint was over/due, so it added a very minuscule amount of stress. Other than that, luckily, since there wasn't money invested, the cost of catching it was only time and stress.

---

## Section 4 — QA Assessment

**How QA actually operated:**  Our QA was very unorganized and continuous; everything was tested on the spot, usually by whoever coded it. We didn't set aside or dedicate a specific time or stage for QA, we didn't write any test cases at all, and the only regression check we had was at the end, before the demo.

**How that compares to your chosen model:**  We sneaked in QA whenever possible instead of giving it its allotted time, as it should be in the V-Model. We didn't write any test cases before writing the code, and then when we did test, it was usually a quick yeah, it works from the developer, and we moved on instead of having something to check for by means of a test case.

**What QA would have looked like under strict adherence:**  Ideally, we should have made the test cases before, so then they could be used as a checklist or requirements for development. Then somebody who is not the developer/programmer would test it using the test cases established from the start.

---

## Section 5 — Team Retrospective on Process

<!-- 
Identify the single most significant gap between how Sprint 1 ran and how it would have run under strict adherence to your chosen model.  

Answer three things:  
1. What was the gap?  
2. What did it cost your team in practice?  
3. If you ran Sprint 2 under strict model adherence, what one change would have the most impact? 
-->

**The gap:** The gap was that we had some requirements established at the start, but we didn't have all, for example we knew we needed to make a Tamagotchi game, but we had originally planned for it to be in the terminal, straight text; it ended up being paired with HTML. We also didn't have any test cases.

**What it cost the team:** This made it super difficult for us to have an exact idea of what needed to happen and what our final product was going to look like. We ended up with a terminal game that shied away in the presence of all the other games. We made a late-minute call to add HTML to make it more visually appealing and hopefully engaging. When our classmates tested it, the html hadnt been implemented successfully yet, so they only saw the terminal game until we had to present, which is when they saw the HTML.

**The one change for Sprint 2:**  I would make sure we have test cases, especially if its game, as it would add structure and checkpoints to keep the team on track with the end goal and what needs to happen. This would also prevent scope creep.

## Contributors:
Karim    **Section Led:** All


